{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras import backend as K\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "#ignore warning messages \n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore') \n",
    "\n",
    "# sns.set()\n",
    "\n",
    "###### Voor Tensorflow-GPU ########\n",
    "\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "train = pd.read_csv(\"Data/train.csv\")\n",
    "test = pd.read_csv(\"Data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = train[\"label\"]\n",
    "Y_test = test[\"label\"]\n",
    "\n",
    "# Drop 'label' column\n",
    "X_train = train.drop(labels = [\"label\"],axis = 1) \n",
    "X_test = test.drop(labels = [\"label\"],axis = 1) \n",
    "\n",
    "# free some space\n",
    "del train \n",
    "del test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape image in 3 dimensions (height = 28px, width = 28px , channel = 1)\n",
    "X_train = X_train.values.reshape(-1,28,28,1)\n",
    "X_test = X_test.values.reshape(-1,28,28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels to one hot vectors (ex : 2 -> [0,0,1,0,0,0,0,0,0,0])\n",
    "Y_train = to_categorical(Y_train, num_classes = 10)\n",
    "Y_test = to_categorical(Y_test, num_classes = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the train and the validation set for the fitting\n",
    "# X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size = 0.1, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "g = plt.imshow(X_train[2], cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_train[2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the CNN model \n",
    "# my CNN architechture is In -> [[Conv2D->relu]*2 -> MaxPool2D -> Dropout]*2 -> Flatten -> Dense -> Dropout -> Out\n",
    "\n",
    "model = Sequential(name=\"mnist_model\")\n",
    "\n",
    "model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n",
    "                 activation ='relu', input_shape = (28,28,1), name=\"mnist_input\"))\n",
    "model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation = \"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation = \"softmax\", name=\"mnist_output\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the optimizer\n",
    "# optimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"mnist_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "mnist_input (Conv2D)         (None, 28, 28, 32)        832       \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 28, 28, 32)        25632     \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 14, 14, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 14, 14, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               803072    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "mnist_output (Dense)         (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 887,530\n",
      "Trainable params: 887,530\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer = \"adam\" , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a learning rate annealer\n",
    "# learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n",
    "#                                             patience=3, \n",
    "#                                             verbose=1, \n",
    "#                                             factor=0.5, \n",
    "#                                             min_lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystopper = EarlyStopping(patience=5, verbose=1)\n",
    "epochs = 100 # Turn epochs to 30 to get 0.9967 accuracy\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without data augmentation i obtained an accuracy of 0.98114\n",
    "#history = model.fit(X_train, Y_train, batch_size = batch_size, epochs = epochs, \n",
    "#          validation_data = (X_val, Y_val), verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With data augmentation to prevent overfitting (accuracy 0.99286)\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        zoom_range = 0.1, # Randomly zoom image \n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=False,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "\n",
    "\n",
    "datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "935/938 [============================>.] - ETA: 0s - loss: 0.1063 - accuracy: 0.9685WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0010s vs `on_test_batch_end` time: 0.0020s). Check your callbacks.\n",
      "938/938 [==============================] - 11s 12ms/step - loss: 0.1060 - accuracy: 0.9686 - val_loss: 0.0453 - val_accuracy: 0.9853\n",
      "Epoch 2/3\n",
      "938/938 [==============================] - 11s 12ms/step - loss: 0.0884 - accuracy: 0.9741 - val_loss: 0.0397 - val_accuracy: 0.9874\n",
      "Epoch 3/3\n",
      "938/938 [==============================] - 11s 12ms/step - loss: 0.0850 - accuracy: 0.9752 - val_loss: 0.0257 - val_accuracy: 0.9923\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "history = model.fit_generator(datagen.flow(X_train,Y_train, batch_size=batch_size),\n",
    "                              epochs = 3, validation_data = (X_test,Y_test),\n",
    "                              verbose = 1, callbacks=[earlystopper])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper right');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dink = [0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 4, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 2, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 62, 117, 88, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 143, 255, 161, 254, 197, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 254, 41, 1, 0, 255, 40, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 113, 253, 42, 0, 168, 114, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 96, 255, 49, 167, 110, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 10, 1, 247, 59, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 5, 15, 249, 2, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 1, 0, 173, 153, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 4, 253, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 239, 159, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 1, 0, 2, 0, 2, 0, 2, 124, 232, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0, 3, 0, 0, 0, 5, 32, 255, 6, 0, 1, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 2, 0, 1, 0, 253, 60, 3, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 3, 0, 197, 148, 6, 0, 3, 123, 9, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 1, 6, 30, 255, 252, 255, 178, 171, 247, 255, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 5, 0, 176, 247, 0, 16, 114, 113, 72, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 3, 1, 0, 0, 2, 0, 0, 6, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "dink = np.array(dink)\n",
    "dink = dink / 255.0\n",
    "g = plt.imshow(dink.reshape(28,28,1), cmap=\"gray\")\n",
    "dink = dink.reshape(-1, 28, 28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "print(dink.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00536442 0.08463486 0.7987229  0.01003878 0.01352202 0.00623416\n",
      " 0.00249375 0.02294049 0.03221908 0.02382957]\n",
      "0 has prob of 0.009999999776482582\n",
      "1 has prob of 0.07999999821186066\n",
      "2 has prob of 0.800000011920929\n",
      "3 has prob of 0.009999999776482582\n",
      "4 has prob of 0.009999999776482582\n",
      "5 has prob of 0.009999999776482582\n",
      "6 has prob of 0.0\n",
      "7 has prob of 0.019999999552965164\n",
      "8 has prob of 0.029999999329447746\n",
      "9 has prob of 0.019999999552965164\n",
      "Closest number is 2\n"
     ]
    }
   ],
   "source": [
    "# predict results\n",
    "results = model.predict(dink)\n",
    "print(results[0])\n",
    "for i in results[0]:\n",
    "    print(f\"{np.where(results[0] == i)[0][0]} has prob of {round(i,2)}\")\n",
    "\n",
    "# select the indix with the maximum probability\n",
    "results = np.argmax(results,axis = 1)\n",
    "print(f\"Closest number is {results[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00536442 0.08463486 0.7987229  0.01003878 0.01352202 0.00623416\n",
      "  0.00249375 0.02294049 0.03221908 0.02382957]]\n"
     ]
    }
   ],
   "source": [
    "results = model.predict(dink)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keras2onnx version is 1.8.0\n"
     ]
    }
   ],
   "source": [
    "# KERAS MODEL OPSLAAN\n",
    "\n",
    "import keras2onnx\n",
    "print(\"keras2onnx version is \"+keras2onnx.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "Processing a keras layer - (mnist_output: <class 'tensorflow.python.keras.layers.core.Dense'>)\n",
      "\toutput: mnist_output/Softmax:0\n",
      "\tinput : dropout_2/cond/Identity:0\n",
      "Processing a keras layer - (dropout_2: <class 'tensorflow.python.keras.layers.core.Dropout'>)\n",
      "\toutput: dropout_2/cond/Identity:0\n",
      "\tinput : dense/Relu:0\n",
      "Processing a keras layer - (dense: <class 'tensorflow.python.keras.layers.core.Dense'>)\n",
      "\toutput: dense/Relu:0\n",
      "\tinput : flatten/Reshape:0\n",
      "Processing a keras layer - (flatten: <class 'tensorflow.python.keras.layers.core.Flatten'>)\n",
      "\toutput: flatten/Reshape:0\n",
      "\tinput : dropout_1/cond/Identity:0\n",
      "Processing a keras layer - (dropout_1: <class 'tensorflow.python.keras.layers.core.Dropout'>)\n",
      "\toutput: dropout_1/cond/Identity:0\n",
      "\tinput : max_pooling2d_1/MaxPool:0\n",
      "Processing a keras layer - (max_pooling2d_1: <class 'tensorflow.python.keras.layers.pooling.MaxPooling2D'>)\n",
      "\toutput: max_pooling2d_1/MaxPool:0\n",
      "\tinput : conv2d_2/Relu:0\n",
      "Processing a keras layer - (conv2d_2: <class 'tensorflow.python.keras.layers.convolutional.Conv2D'>)\n",
      "\toutput: conv2d_2/Relu:0\n",
      "\tinput : conv2d_1/Relu:0\n",
      "Processing a keras layer - (conv2d_1: <class 'tensorflow.python.keras.layers.convolutional.Conv2D'>)\n",
      "\toutput: conv2d_1/Relu:0\n",
      "\tinput : dropout/cond/Identity:0\n",
      "Processing a keras layer - (dropout: <class 'tensorflow.python.keras.layers.core.Dropout'>)\n",
      "\toutput: dropout/cond/Identity:0\n",
      "\tinput : max_pooling2d/MaxPool:0\n",
      "Processing a keras layer - (max_pooling2d: <class 'tensorflow.python.keras.layers.pooling.MaxPooling2D'>)\n",
      "\toutput: max_pooling2d/MaxPool:0\n",
      "\tinput : conv2d/Relu:0\n",
      "Processing a keras layer - (conv2d: <class 'tensorflow.python.keras.layers.convolutional.Conv2D'>)\n",
      "\toutput: conv2d/Relu:0\n",
      "\tinput : mnist_input/Relu:0\n",
      "Processing a keras layer - (mnist_input: <class 'tensorflow.python.keras.layers.convolutional.Conv2D'>)\n",
      "\toutput: mnist_input/Relu:0\n",
      "\tinput : mnist_input_input:0\n",
      "var: mnist_input_input\n",
      "var: mnist_input_input:0\n",
      "var: mnist_input_input:01\n",
      "var: mnist_input/Relu:0\n",
      "var: conv2d/Relu:0\n",
      "var: max_pooling2d/MaxPool:0\n",
      "var: dropout/cond/Identity:0\n",
      "var: conv2d_1/Relu:0\n",
      "var: conv2d_2/Relu:0\n",
      "var: max_pooling2d_1/MaxPool:0\n",
      "var: dropout_1/cond/Identity:0\n",
      "var: flatten/Reshape:0\n",
      "var: dense/Relu:0\n",
      "var: dropout_2/cond/Identity:0\n",
      "var: mnist_output/Softmax:01\n",
      "var: mnist_output/Softmax:0\n",
      "var: mnist_output\n",
      "Converting the operator (Identity): Identity\n",
      "Converting the operator (Identity1): Identity\n",
      "Converting the operator (Identity2): Identity\n",
      "Converting the operator (mnist_output): <class 'tensorflow.python.keras.layers.core.Dense'>\n",
      "Converting the operator (dropout_2): <class 'tensorflow.python.keras.layers.core.Dropout'>\n",
      "Converting the operator (keras_learning_phase/input): Const\n",
      "Converting the operator (dense): <class 'tensorflow.python.keras.layers.core.Dense'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"mnist_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "mnist_input (Conv2D)         (None, 28, 28, 32)        832       \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 28, 28, 32)        25632     \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 14, 14, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 14, 14, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               803072    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "mnist_output (Dense)         (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 887,530\n",
      "Trainable params: 887,530\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting the operator (flatten): <class 'tensorflow.python.keras.layers.core.Flatten'>\n",
      "Converting the operator (flatten/Const): Const\n",
      "Converting the operator (dropout_1): <class 'tensorflow.python.keras.layers.core.Dropout'>\n",
      "Converting the operator (max_pooling2d_1): <class 'tensorflow.python.keras.layers.pooling.MaxPooling2D'>\n",
      "Converting the operator (conv2d_2): <class 'tensorflow.python.keras.layers.convolutional.Conv2D'>\n",
      "Converting the operator (conv2d_1): <class 'tensorflow.python.keras.layers.convolutional.Conv2D'>\n",
      "Converting the operator (dropout): <class 'tensorflow.python.keras.layers.core.Dropout'>\n",
      "Converting the operator (max_pooling2d): <class 'tensorflow.python.keras.layers.pooling.MaxPooling2D'>\n",
      "Converting the operator (conv2d): <class 'tensorflow.python.keras.layers.convolutional.Conv2D'>\n",
      "Converting the operator (mnist_input): <class 'tensorflow.python.keras.layers.convolutional.Conv2D'>\n",
      "Converting the operator (Identity3): Identity\n"
     ]
    }
   ],
   "source": [
    "# convert to onnx model\n",
    "onnx_model = keras2onnx.convert_keras(model, 'mnist-onnx-v2', debug_mode=1)\n",
    "output_model_path = \"./mnist-onnx-v2.onnx\"\n",
    "# and save the model in ONNX format\n",
    "keras2onnx.save_model(onnx_model, output_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = onnxruntime.InferenceSession(\"mnist-onnx-v2.onnx\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnist_input_input\n",
      "mnist_output\n"
     ]
    }
   ],
   "source": [
    "input_name = session.get_inputs()[0].name\n",
    "output_name = session.get_outputs()[0].name\n",
    "print(input_name)\n",
    "print(output_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0.00536442, 0.08463484, 0.79872304, 0.01003877, 0.01352201,\n",
      "        0.00623415, 0.00249375, 0.02294048, 0.03221906, 0.02382956]],\n",
      "      dtype=float32)]\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "result = session.run([output_name], {input_name: dink.astype('float32')})\n",
    "print(result)\n",
    "prediction=int(np.argmax(np.array(result).squeeze(), axis=0))\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"mnist_input_input:0\", shape=(None, 28, 28, 1), dtype=float32)\n",
      "mnist_output/Softmax:0\n"
     ]
    }
   ],
   "source": [
    "print(model.inputs[0])\n",
    "print(model.output.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict the test set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      1195\n",
      "           1       1.00      0.99      1.00      1352\n",
      "           2       0.99      1.00      0.99      1157\n",
      "           3       0.99      0.99      0.99      1258\n",
      "           4       0.99      1.00      0.99      1140\n",
      "           5       1.00      0.99      0.99      1076\n",
      "           6       0.99      1.00      1.00      1167\n",
      "           7       0.99      0.99      0.99      1268\n",
      "           8       0.99      0.99      0.99      1174\n",
      "           9       0.99      0.97      0.98      1213\n",
      "\n",
      "    accuracy                           0.99     12000\n",
      "   macro avg       0.99      0.99      0.99     12000\n",
      "weighted avg       0.99      0.99      0.99     12000\n",
      "\n",
      "[[1188    0    0    0    0    0    6    0    1    0]\n",
      " [   1 1343    1    1    0    0    1    5    0    0]\n",
      " [   0    0 1155    2    0    0    0    0    0    0]\n",
      " [   0    0    3 1251    0    1    0    1    1    1]\n",
      " [   0    2    0    0 1135    0    0    0    0    3]\n",
      " [   0    0    0    4    0 1067    3    0    2    0]\n",
      " [   1    0    0    0    0    0 1166    0    0    0]\n",
      " [   0    0    8    1    2    0    0 1255    1    1]\n",
      " [   0    0    2    0    1    1    0    0 1168    2]\n",
      " [   4    0    0    3   14    1    0    6    5 1180]]\n",
      "99.23333333333333\n"
     ]
    }
   ],
   "source": [
    "print('Predict the test set')\n",
    "predict = model.predict(X_test, batch_size=batch_size)\n",
    "predict = np.argmax(predict,axis=1)\n",
    "class_report = classification_report(Y_test.argmax(axis=1),predict,target_names=[str(i) for i in range(10)])\n",
    "print(class_report)\n",
    "#run.log(\"Classification report\", class_report)\n",
    "\n",
    "cf = confusion_matrix(Y_test.argmax(axis=1), predict)\n",
    "#run.log(\"Confusion Matrix\", cf)\n",
    "\n",
    "print(cf)\n",
    "acc = accuracy_score(Y_test.argmax(axis=1), predict) * 100\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = model.predict(X_test, batch_size=batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
